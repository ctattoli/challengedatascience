{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "65316ec6b67542caabacd151424495e1",
   "source": [
    "# Similitud entre celulares \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "d2355515b5e94d0e98dca9cae759379c",
   "source": [
    "gcloud auth application-default revoke\n",
    "\n",
    "gcloud config get-value project\n",
    "\n",
    "gcloud config get-value project\n",
    "\n",
    "gcloud auth login\n",
    "\n",
    "gcloud config set project meli-bi-data\n",
    "\n",
    "gcloud config list\n",
    "\n",
    "gcloud auth application-default login\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "9a33ad69f23e4208a63d0b7b0e288563",
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -q scikit-learn pandas numpy matplotlib rapidfuzz python-Levenshtein\n",
    "\n",
    "\n",
    "import IPython, time\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "04c6fd5daefa4b7da1df1da15700bf18",
   "source": [
    "# !pip install -q pandas numpy scikit-learn matplotlib rapidfuzz python-Levenshtein openai\n",
    "import sys, os, math, re, json, time, random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "except Exception:\n",
    "    class _FauxFuzz:\n",
    "        @staticmethod\n",
    "        def ratio(a,b):\n",
    "            return 100 if a==b else max(0, 100-len(set(a.split())^set(b.split()))*15)\n",
    "    fuzz = _FauxFuzz()\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "03f473c4787e4013be0f0b606fb051e6",
   "source": [
    "\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "\n",
    "creds, project = google.auth.default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "client = bigquery.Client(project=project, credentials=creds)\n",
    "\n",
    "\n",
    "client.query(\"SELECT 1 AS ok\", location=\"US\").result()\n",
    "print(\"OK\")\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT 1 AS ok\n",
    "\"\"\"\n",
    "job = client.query(query, location=\"US\")  # <-- location acá\n",
    "df = job.to_dataframe()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "9efc736e283f4a48be5755a105649617",
   "source": [
    "from google.cloud import bigquery_storage\n",
    "\n",
    "bqstorage_client = bigquery_storage.BigQueryReadClient()\n",
    "df = job.to_dataframe(bqstorage_client=bqstorage_client)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "03f6bd63d04f4f159305081ac75ad33e",
   "source": [
    "PROJECT    = \"meli-bi-data\"\n",
    "LOCATION   = \"US\"\n",
    "FULL_NAME  = \"`meli-bi-data.WHOWNER.SBOX_SELLERSMP.CT_CELS_MLM`\"  # tabla/vista fuente\n",
    "\n",
    "OUT_DIR    = \"./eda_outputs_cels\"\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "e0df09294d994eceb0b420576a5e037d",
   "source": [
    "\n",
    "# =========================\n",
    "# Imports y cliente BigQuery\n",
    "# =========================\n",
    "import os, warnings\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (10, 4), \"axes.grid\": True})\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "creds, _ = google.auth.default(scopes=SCOPES)\n",
    "bq = bigquery.Client(project=PROJECT, credentials=creds, location=LOCATION)\n",
    "\n",
    "try:\n",
    "    from google.cloud import bigquery_storage\n",
    "    bqstorage_client = bigquery_storage.BigQueryReadClient(credentials=creds)\n",
    "except Exception:\n",
    "    bqstorage_client = None\n",
    "\n",
    "print(\"✅ Cliente BigQuery listo. Proyecto de billing:\", bq.project)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "4d623dbe96fc429ab260ff2fe6b22595",
   "source": [
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "from typing import List\n",
    "\n",
    "def dry_run_bytes(sql: str, params=None) -> int:\n",
    "    cfg = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False,\n",
    "                                  query_parameters=params or [])\n",
    "    job = bq.query(sql, job_config=cfg)\n",
    "    return job.total_bytes_processed\n",
    "\n",
    "def run_df(sql: str, params=None) -> pd.DataFrame:\n",
    "    params = params or []\n",
    "    print(f\"Bytes (dry run): {dry_run_bytes(sql, params):,}\")\n",
    "    job_cfg = bigquery.QueryJobConfig(query_parameters=params)\n",
    "    return bq.query(sql, job_config=job_cfg).to_dataframe(bqstorage_client=bqstorage_client)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "36dbdc6610f44193b6914697de1984c1",
   "source": [
    "1. REDUCCIÓN DE DIMENSIONALIDAD"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "631e62f27c9144898492ba52533aa76c",
   "source": [
    "\n",
    "SQL_BASE = f\"\"\"\n",
    "\n",
    "  SELECT\n",
    "    ITE_ITEM_ID,\tITE_ITEM_TITLE,\tITE_ITEM_CATALOG_PRODUCT_ID,\tITE_ITEM_CURRENT_PRICE,\tMODEL,\tITEM_CONDITION,\tBRAND,\tRAM,\tCOLOR,\tBATTERY_CAPACITY,\tINTERNAL_MEMORY,\tMAIN_FRONT_CAMERA_RESOLUTION,\tMAIN_REAR_CAMERA_RESOLUTION,\tQTY_VISITS\n",
    "  FROM meli-bi-data.SBOX_SELLERSMP.CELS_MLM AS t\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "_ = dry_run_bytes(SQL_BASE)\n",
    "print(f\"Dry-run bytes (pipeline base): {_:,}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "ddcc8eb0e8cb4925869fdc4b74621a60",
   "source": [
    "import math\n",
    "\n",
    "def n_for_prop(p=0.05, eps=0.0025, z=2.58, N=69951, deff=1.0):\n",
    "    \"\"\"Tamaño muestral para una proporción con (opcional) corrección por población finita y design effect.\"\"\"\n",
    "    n0 = (z**2 * p * (1-p)) / (eps**2)     # Cochran (población infinita)\n",
    "    n0 *= deff                             # efecto de diseño\n",
    "    if not N:                              # sin FPC\n",
    "        return math.ceil(n0)\n",
    "    n = n0 / (1 + (n0 - 1) / N)            # FPC\n",
    "    return math.ceil(n)\n",
    "\n",
    "# Ejemplos\n",
    "print(\"n0 infinita:\", n_for_prop(0.05, 0.0025, 2.58))\n",
    "print(\"N con FPC:\", n_for_prop(0.05, 0.0025, 2.58, N=69_951))\n",
    "print(\"N + diseño (DEFF=1.2):\", n_for_prop(0.05, 0.0025, 2.58, N=69_951, deff=1.2))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "1d4c9b55bda54da582e6613764f083e5",
   "source": [
    "#REDUCCIÓN DE DIMENSIONALIDAD\n",
    "\n",
    "PROJECT   = \"meli-bi-data\"\n",
    "LOCATION  = \"US\"\n",
    "TABLE_FULL = \"`meli-bi-data.SBOX_SELLERSMP.CELS_MLM`\"  # ajustá si tu ruta es distinta\n",
    "\n",
    "PCT_USERS  = 70             \n",
    "\n",
    "USE_TABLESAMPLE   = True    \n",
    "TABLESAMPLE_PCT   = 70    \n",
    "TABLESAMPLE_SEED  = 42    \n",
    "\n",
    "# =========================\n",
    "# Cliente y helpers\n",
    "# =========================\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "creds, _ = google.auth.default(scopes=SCOPES)\n",
    "client = bigquery.Client(project=PROJECT, credentials=creds, location=LOCATION)\n",
    "\n",
    "try:\n",
    "    from google.cloud import bigquery_storage\n",
    "    bqstorage_client = bigquery_storage.BigQueryReadClient(credentials=creds)\n",
    "except Exception:\n",
    "    bqstorage_client = None\n",
    "\n",
    "def human_bytes(n):\n",
    "    for unit in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if n < 1024: return f\"{n:.2f} {unit}\"\n",
    "        n /= 1024\n",
    "    return f\"{n:.2f} PB\"\n",
    "\n",
    "\n",
    "TABLESAMPLE_CLAUSE = (\n",
    "    f\"TABLESAMPLE SYSTEM ({TABLESAMPLE_PCT} PERCENT) \"\n",
    "    if USE_TABLESAMPLE else \"\"\n",
    ")\n",
    "\n",
    "SQL = f\"\"\"\n",
    "WITH sampled AS (\n",
    "  SELECT *\n",
    "  FROM {TABLE_FULL}\n",
    "  {TABLESAMPLE_CLAUSE}\n",
    "),\n",
    "base AS (\n",
    "  SELECT\n",
    "    ITE_ITEM_ID,\tITE_ITEM_TITLE,\tITE_ITEM_CATALOG_PRODUCT_ID,\tITE_ITEM_CURRENT_PRICE,\tMODEL,\tITEM_CONDITION,\tBRAND,\tRAM,\tCOLOR,\tBATTERY_CAPACITY,\tINTERNAL_MEMORY,\tMAIN_FRONT_CAMERA_RESOLUTION,\tMAIN_REAR_CAMERA_RESOLUTION,\tQTY_VISITS\n",
    "  FROM sampled AS t\n",
    "  WHERE \n",
    "    -- Muestreo estable por usuario (preserva balance T/C y es reproducible)\n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(t.ITE_ITEM_ID AS STRING))), 100) < @pct\n",
    ")\n",
    "SELECT *\n",
    "FROM base\n",
    "\"\"\"\n",
    "\n",
    "params = [\n",
    "\n",
    "    bigquery.ScalarQueryParameter(\"pct\",\"INT64\",       PCT_USERS),\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Dry run de bytes\n",
    "# =========================\n",
    "dry_cfg = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False, query_parameters=params)\n",
    "dry_job = client.query(SQL, job_config=dry_cfg)\n",
    "print(\"Dry-run:\", human_bytes(dry_job.total_bytes_processed))\n",
    "\n",
    "\n",
    "df = client.query(SQL, job_config=bigquery.QueryJobConfig(query_parameters=params)).to_dataframe(\n",
    "        bqstorage_client=bqstorage_client\n",
    ")\n",
    "print(\"Shape DF:\", df.shape)\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "1c9cde83a1f34c66a38df921f680c5b2",
   "source": [
    "2. TESTS DE REPRESENTATIVIDAD"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "201daa9ff50a4881ad23688817436086",
   "source": [
    "# =========================\n",
    "#TEST DE REPRESENTATIVIDAD \n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, warnings, os\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "\n",
    "if 'df' not in globals():\n",
    "    raise NameError(\"No encontré 'df'. Asegurate de tener tu muestra en una variable llamada df.\")\n",
    "SAMPLE = df.copy()\n",
    "\n",
    "\n",
    "POP_BASE = globals().get('df_pop', None) or globals().get('df_bq', None)\n",
    "\n",
    "\n",
    "if POP_BASE is None and ('TABLE_FULL' in globals()):\n",
    "    try:\n",
    "        from google.cloud import bigquery\n",
    "        import google.auth\n",
    "        PROJECT  = globals().get('PROJECT', os.getenv('BQ_PROJECT', 'meli-bi-data'))\n",
    "        LOCATION = globals().get('LOCATION', os.getenv('BQ_LOCATION', 'US'))\n",
    "        SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "        creds, _ = google.auth.default(scopes=SCOPES)\n",
    "        bq_client = bigquery.Client(project=PROJECT, credentials=creds, location=LOCATION)\n",
    "\n",
    "     \n",
    "        SQL_POP = f\"\"\"\n",
    "        SELECT\n",
    "          ITE_ITEM_ID, ITE_ITEM_TITLE, ITE_ITEM_CATALOG_PRODUCT_ID, ITE_ITEM_CURRENT_PRICE,\n",
    "          MODEL, ITEM_CONDITION, BRAND, RAM, COLOR, BATTERY_CAPACITY, INTERNAL_MEMORY,\n",
    "          MAIN_FRONT_CAMERA_RESOLUTION, MAIN_REAR_CAMERA_RESOLUTION, QTY_VISITS\n",
    "        FROM {TABLE_FULL}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            dry = bq_client.query(SQL_POP, job_config=bigquery.QueryJobConfig(dry_run=True, use_query_cache=True))\n",
    "            print(f\"POP BigQuery dry-run bytes: {dry.total_bytes_processed:,}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        POP_BASE = bq_client.query(SQL_POP).to_dataframe()\n",
    "        print(\"POP obtenido desde BigQuery:\", POP_BASE.shape)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ No pude traer POP desde BigQuery:\", e)\n",
    "\n",
    "\n",
    "if POP_BASE is None:\n",
    "    print(\"⚠️ Fallback: usando POP = SAMPLE (los tests darán match perfecto; solo útil para debug).\")\n",
    "    POP_BASE = SAMPLE.copy()\n",
    "\n",
    "print(\"SAMPLE shape:\", SAMPLE.shape, \"| POP shape:\", POP_BASE.shape)\n",
    "\n",
    "\n",
    "colmap = {\n",
    "    'ITE_ITEM_ID':                  'ITE_ITEM_ID',\n",
    "    'ITE_ITEM_TITLE':               'ITE_ITEM_TITLE',\n",
    "    'ITE_ITEM_CATALOG_PRODUCT_ID':  'ITE_ITEM_CATALOG_PRODUCT_ID',\n",
    "    'ITE_ITEM_CURRENT_PRICE':       'ITE_ITEM_CURRENT_PRICE',\n",
    "    'MODEL':                        'MODEL',\n",
    "    'ITEM_CONDITION':               'ITEM_CONDITION',\n",
    "    'BRAND':                        'BRAND',\n",
    "    'RAM':                          'RAM',\n",
    "    'COLOR':                        'COLOR',\n",
    "    'BATTERY_CAPACITY':             'BATTERY_CAPACITY',\n",
    "    'INTERNAL_MEMORY':              'INTERNAL_MEMORY',\n",
    "    'MAIN_FRONT_CAMERA_RESOLUTION': 'MAIN_FRONT_CAMERA_RESOLUTION',\n",
    "    'MAIN_REAR_CAMERA_RESOLUTION':  'MAIN_REAR_CAMERA_RESOLUTION',\n",
    "    'QTY_VISITS':                   'QTY_VISITS',\n",
    "}\n",
    "\n",
    "def parse_gb(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r'(\\d{1,4})\\s*gb', str(x).lower())\n",
    "    return int(m.group(1)) if m else pd.to_numeric(x, errors='coerce')\n",
    "\n",
    "def apply_colmap(df_in: pd.DataFrame, colmap: dict) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "   \n",
    "    rename_dict = {real: canon for canon, real in colmap.items() if real in df_out.columns and real != canon}\n",
    "    if rename_dict:\n",
    "        df_out = df_out.rename(columns=rename_dict)\n",
    " \n",
    "    for canon in colmap.keys():\n",
    "        if canon not in df_out.columns:\n",
    "            df_out[canon] = np.nan\n",
    "    \n",
    "    df_out['ITE_ITEM_CURRENT_PRICE'] = pd.to_numeric(df_out['ITE_ITEM_CURRENT_PRICE'], errors='coerce')\n",
    "    df_out['QTY_VISITS'] = pd.to_numeric(df_out['QTY_VISITS'], errors='coerce')\n",
    "    \n",
    "    df_out['RAM_GB'] = df_out['RAM'].apply(parse_gb)\n",
    "    df_out['INTERNAL_MEMORY_GB'] = df_out['INTERNAL_MEMORY'].apply(parse_gb)\n",
    "    return df_out\n",
    "\n",
    "POP = apply_colmap(POP_BASE, colmap)\n",
    "SAMPLE = apply_colmap(SAMPLE, colmap)\n",
    "\n",
    "\n",
    "NUMERIC_COLS = [c for c in ['ITE_ITEM_CURRENT_PRICE','QTY_VISITS'] if c in POP.columns and c in SAMPLE.columns]\n",
    "CATEG_COLS   = [c for c in ['BRAND','ITEM_CONDITION','COLOR','MODEL'] if c in POP.columns and c in SAMPLE.columns]\n",
    "if not NUMERIC_COLS and not CATEG_COLS:\n",
    "    raise ValueError(\"No hay columnas compatibles entre POP y SAMPLE para testear.\")\n",
    "\n",
    "\n",
    "def standardized_mean_diff(pop, samp):\n",
    "    pop = np.asarray(pop, dtype=float); samp = np.asarray(samp, dtype=float)\n",
    "    mu_p, mu_s = np.nanmean(pop), np.nanmean(samp)\n",
    "    s_p, s_s = np.nanstd(pop, ddof=1), np.nanstd(samp, ddof=1)\n",
    "    s_pooled = np.sqrt(((s_p**2)+(s_s**2))/2)\n",
    "    if s_pooled == 0 or np.isnan(s_pooled): return 0.0\n",
    "    return (mu_s - mu_p) / s_pooled\n",
    "\n",
    "def js_divergence(pop_counts: dict, samp_counts: dict):\n",
    "    keys = sorted(set(pop_counts.keys()) | set(samp_counts.keys()))\n",
    "    p = np.array([pop_counts.get(k,0) for k in keys], dtype=float)\n",
    "    q = np.array([samp_counts.get(k,0) for k in keys], dtype=float)\n",
    "    if p.sum()==0 or q.sum()==0: return np.nan\n",
    "    p /= p.sum(); q /= q.sum()\n",
    "    return float(jensenshannon(p, q)**2)  # distancia^2 = divergencia\n",
    "\n",
    "def compare_distributions(POP: pd.DataFrame, SAMPLE: pd.DataFrame):\n",
    "    rows = []\n",
    "    # Numericos: KS, t-test, SMD\n",
    "    for col in NUMERIC_COLS:\n",
    "        p = pd.to_numeric(POP[col], errors='coerce').dropna()\n",
    "        s = pd.to_numeric(SAMPLE[col], errors='coerce').dropna()\n",
    "        if len(p)>2 and len(s)>2:\n",
    "            # KS compatible con SciPy nuevas/viejas\n",
    "            try:\n",
    "                ks = stats.ks_2samp(p, s, alternative='two-sided', method='auto')\n",
    "            except TypeError:\n",
    "                ks = stats.ks_2samp(p, s, alternative='two-sided')\n",
    "            tt = stats.ttest_ind(p, s, equal_var=False, nan_policy='omit')\n",
    "            smd = standardized_mean_diff(p, s)\n",
    "            rows.append({'feature':col,'type':'numeric','n_pop':len(p),'n_samp':len(s),\n",
    "                         'ks_stat':round(ks.statistic,4),'ks_p':round(ks.pvalue,4),\n",
    "                         't_stat':round(tt.statistic,4),'t_p':round(tt.pvalue,4),\n",
    "                         'smd':round(smd,4)})\n",
    "        else:\n",
    "            rows.append({'feature':col,'type':'numeric','n_pop':len(p),'n_samp':len(s),\n",
    "                         'ks_stat':None,'ks_p':None,'t_stat':None,'t_p':None,'smd':None})\n",
    "    # Categóricos: Chi2 + JS\n",
    "    for col in CATEG_COLS:\n",
    "        p_counts = POP[col].fillna('NA').astype(str).value_counts().to_dict()\n",
    "        s_counts = SAMPLE[col].fillna('NA').astype(str).value_counts().to_dict()\n",
    "        cats = sorted(set(p_counts.keys()) | set(s_counts.keys()))\n",
    "        p_obs = np.array([p_counts.get(c,0) for c in cats], dtype=float)\n",
    "        s_obs = np.array([s_counts.get(c,0) for c in cats], dtype=float)\n",
    "        if p_obs.sum()==0 or s_obs.sum()==0:\n",
    "            chi2_stat = chi2_p = None\n",
    "        else:\n",
    "            expected = (p_obs / p_obs.sum()) * s_obs.sum()\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                chi2 = stats.chisquare(s_obs, f_exp=expected)\n",
    "            chi2_stat = None if np.isnan(chi2.statistic) else float(chi2.statistic)\n",
    "            chi2_p    = None if np.isnan(chi2.pvalue)   else float(chi2.pvalue)\n",
    "        jsd = js_divergence(p_counts, s_counts)\n",
    "        rows.append({'feature':col,'type':'categorical','n_categories':len(cats),\n",
    "                     'chi2_stat':None if chi2_stat is None else round(chi2_stat,4),\n",
    "                     'chi2_p':None if chi2_p is None else round(chi2_p,4),\n",
    "                     'js_divergence':None if (jsd is None or np.isnan(jsd)) else round(jsd,6)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    TESTS = compare_distributions(POP, SAMPLE)\n",
    "\n",
    "print(\"Resumen (p>0.05 ⇒ sin diferencias significativas; SMD<0.1 ⇒ diferencia pequeña)\")\n",
    "try:\n",
    "    display(TESTS.sort_values(['type','feature']).reset_index(drop=True))\n",
    "except Exception:\n",
    "    print(TESTS.sort_values(['type','feature']).reset_index(drop=True).to_string(index=False))\n",
    "\n",
    "    \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "df7aefe45f5245a1aed6a6924d5db13e",
   "source": [
    "#SMR\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "\n",
    "NUMERIC_COLS = [c for c in ['ITE_ITEM_CURRENT_PRICE','QTY_VISITS'] if c in POP.columns and c in SAMPLE.columns]\n",
    "CATEG_COLS   = [c for c in ['BRAND','ITEM_CONDITION','COLOR','MODEL'] if c in POP.columns and c in SAMPLE.columns]\n",
    "\n",
    "def smr_numeric(POP, SAMPLE, cols):\n",
    "    rows=[]\n",
    "    for c in cols:\n",
    "        p = pd.to_numeric(POP[c], errors='coerce').dropna()\n",
    "        s = pd.to_numeric(SAMPLE[c], errors='coerce').dropna()\n",
    "        if len(p)==0 or len(s)==0: \n",
    "            rows.append({'feature':c,'mu_pop':np.nan,'mu_samp':np.nan,'smr_mean':np.nan})\n",
    "            continue\n",
    "        mu_p, mu_s = p.mean(), s.mean()\n",
    "        smr = (mu_s / mu_p) if mu_p not in [0, np.nan] else np.nan\n",
    "        rows.append({'feature':c, 'mu_pop':mu_p, 'mu_samp':mu_s, 'smr_mean':smr})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def smr_categorical(POP, SAMPLE, cols, eps=1e-9, topk=15):\n",
    "    details_top = []\n",
    "    summaries   = []\n",
    "    for c in cols:\n",
    "        \n",
    "        p_counts = POP[c].fillna('NA').astype(str).value_counts()\n",
    "        s_counts = SAMPLE[c].fillna('NA').astype(str).value_counts()\n",
    "        cats = sorted(set(p_counts.index) | set(s_counts.index))\n",
    "        P = p_counts.reindex(cats, fill_value=0).astype(float)\n",
    "        S = s_counts.reindex(cats, fill_value=0).astype(float)\n",
    "      \n",
    "        Pp = (P + eps) / (P.sum() + eps*len(cats))\n",
    "        Sp = (S + eps) / (S.sum() + eps*len(cats))\n",
    "        ratio = Sp / Pp\n",
    "        abs_pp_diff = (Sp - Pp).abs()\n",
    "        abs_log_ratio = np.abs(np.log(ratio))\n",
    "\n",
    "        df_all = pd.DataFrame({\n",
    "            'feature': c,\n",
    "            'category': cats,\n",
    "            'pop_prop': Pp.values,\n",
    "            'samp_prop': Sp.values,\n",
    "            'ratio': ratio.values,\n",
    "            'abs_pp_diff': abs_pp_diff.values,\n",
    "            'abs_log_ratio': abs_log_ratio\n",
    "        })\n",
    "     \n",
    "        summaries.append({\n",
    "            'feature': c,\n",
    "            'mean_abs_pp_diff_w': float((df_all['abs_pp_diff'] * df_all['pop_prop']).sum()),  # promedio ponderado por pop\n",
    "            'max_abs_pp_diff': float(df_all['abs_pp_diff'].max()),\n",
    "            'mean_abs_log_ratio_w': float((df_all['abs_log_ratio'] * df_all['pop_prop']).sum()),\n",
    "            'max_ratio': float(df_all['ratio'].max()),\n",
    "            'min_ratio': float(df_all['ratio'].min()),\n",
    "            'top_category': df_all.sort_values('abs_log_ratio', ascending=False).iloc[0]['category']\n",
    "        })\n",
    "       \n",
    "        details_top.append(df_all.sort_values('abs_log_ratio', ascending=False).head(topk))\n",
    "\n",
    "    SMR_CAT_TOP = pd.concat(details_top, ignore_index=True) if details_top else pd.DataFrame()\n",
    "    SMR_CAT_SUMMARY = pd.DataFrame(summaries)\n",
    "    return SMR_CAT_TOP, SMR_CAT_SUMMARY\n",
    "\n",
    "\n",
    "SMR_NUM = smr_numeric(POP, SAMPLE, NUMERIC_COLS)\n",
    "SMR_CAT_TOP, SMR_CAT_SUMMARY = smr_categorical(POP, SAMPLE, CATEG_COLS, eps=1e-9, topk=15)\n",
    "\n",
    "print(\"SMR numéricos (mean_samp / mean_pop):\")\n",
    "display(SMR_NUM)\n",
    "\n",
    "print(\"Resumen SMR categóricos (valores cercanos a 1/0 indican buena representación):\")\n",
    "display(SMR_CAT_SUMMARY.sort_values('feature'))\n",
    "\n",
    "print(\"Top categorías con mayor desvío (para revisión):\")\n",
    "display(SMR_CAT_TOP.head(30))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "275cf92fb5fc49519cac50aad4dc7670",
   "source": [
    "## SMR AJUSTADO\n",
    "\n",
    "MIN_POP_PROP = 0.001\n",
    "\n",
    "def smr_categorical_thresholded(POP, SAMPLE, cols, min_pop_prop=MIN_POP_PROP, eps=1e-9, topk=15):\n",
    "    import numpy as np, pandas as pd\n",
    "    details_top, summaries = [], []\n",
    "    for c in cols:\n",
    "        p_counts = POP[c].fillna('NA').astype(str).value_counts()\n",
    "        s_counts = SAMPLE[c].fillna('NA').astype(str).value_counts()\n",
    "        cats = sorted(set(p_counts.index) | set(s_counts.index))\n",
    "        P = p_counts.reindex(cats, fill_value=0).astype(float)\n",
    "        S = s_counts.reindex(cats, fill_value=0).astype(float)\n",
    "\n",
    "        # proporciones + smoothing\n",
    "        Pp = (P + eps) / (P.sum() + eps*len(cats))\n",
    "        Sp = (S + eps) / (S.sum() + eps*len(cats))\n",
    "\n",
    "        df_all = pd.DataFrame({'feature': c, 'category': cats, 'pop_prop': Pp, 'samp_prop': Sp})\n",
    "        df_all = df_all[df_all['pop_prop'] >= min_pop_prop].copy()  # <-- filtra colas\n",
    "        if df_all.empty:\n",
    "            continue\n",
    "\n",
    "        df_all['ratio'] = df_all['samp_prop'] / df_all['pop_prop']\n",
    "        df_all['abs_pp_diff'] = (df_all['samp_prop'] - df_all['pop_prop']).abs()\n",
    "        df_all['abs_log_ratio'] = np.abs(np.log(df_all['ratio']))\n",
    "\n",
    "        summaries.append({\n",
    "            'feature': c,\n",
    "            'coverage_pop': float(df_all['pop_prop'].sum()),  # masa cubierta tras el filtro\n",
    "            'mean_abs_pp_diff_w': float((df_all['abs_pp_diff'] * df_all['pop_prop']).sum()),\n",
    "            'max_abs_pp_diff': float(df_all['abs_pp_diff'].max()),\n",
    "            'mean_abs_log_ratio_w': float((df_all['abs_log_ratio'] * df_all['pop_prop']).sum()),\n",
    "            'max_ratio': float(df_all['ratio'].max()),\n",
    "            'min_ratio': float(df_all['ratio'].min()),\n",
    "            'top_category': df_all.sort_values('abs_log_ratio', ascending=False).iloc[0]['category']\n",
    "        })\n",
    "        details_top.append(df_all.sort_values('abs_log_ratio', ascending=False).head(topk))\n",
    "\n",
    "    SMR_CAT_SUMMARY_T = pd.DataFrame(summaries)\n",
    "    SMR_CAT_TOP_T = pd.concat(details_top, ignore_index=True) if details_top else pd.DataFrame()\n",
    "    return SMR_CAT_TOP_T, SMR_CAT_SUMMARY_T\n",
    "\n",
    "SMR_CAT_TOP_T, SMR_CAT_SUMMARY_T = smr_categorical_thresholded(POP, SAMPLE, ['BRAND','ITEM_CONDITION','COLOR','MODEL'])\n",
    "\n",
    "display(SMR_CAT_SUMMARY_T.sort_values('feature'))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "7673f213086043a7a9801ddaf20c3480",
   "source": [
    "## TAMAÑO DEL EFECTO CREAMERS V\n",
    "import numpy as np, pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def cramers_v_from_counts(p_counts, s_counts):\n",
    "    cats = sorted(set(p_counts) | set(s_counts))\n",
    "    p_obs = np.array([p_counts.get(c,0) for c in cats], dtype=float)\n",
    "    s_obs = np.array([s_counts.get(c,0) for c in cats], dtype=float)\n",
    "    n = s_obs.sum()\n",
    "    if n == 0: return np.nan\n",
    "    expected = (p_obs / p_obs.sum()) * n\n",
    "    chi2 = ((s_obs - expected)**2 / np.where(expected==0, 1, expected)).sum()\n",
    "    k = len(cats)\n",
    "    V = np.sqrt(chi2 / (n * (k-1))) if k > 1 else 0.0\n",
    "    return V\n",
    "\n",
    "out = []\n",
    "for col in ['BRAND','COLOR','ITEM_CONDITION','MODEL']:\n",
    "    p_counts = POP[col].fillna('NA').astype(str).value_counts().to_dict()\n",
    "    s_counts = SAMPLE[col].fillna('NA').astype(str).value_counts().to_dict()\n",
    "    V = cramers_v_from_counts(p_counts, s_counts)\n",
    "    out.append({'feature':col, 'cramers_v': round(float(V), 4)})\n",
    "pd.DataFrame(out)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cb331c6eba5844959da9c375c6a908e7",
   "source": [
    "3. RESUMEN ESTADISTICO"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "dd0f0825a1ea4e7b98e09bbeacbb8094",
   "source": [
    "\n",
    "def numeric_summary(col):\n",
    "    p = pd.to_numeric(POP[col], errors='coerce')\n",
    "    s = pd.to_numeric(SAMPLE[col], errors='coerce')\n",
    "    return pd.DataFrame({\n",
    "        'feature':[col],\n",
    "        'pop_mean':[p.mean()], 'samp_mean':[s.mean()], 'diff_mean':[s.mean()-p.mean()],\n",
    "        'pop_median':[p.median()], 'samp_median':[s.median()], 'diff_median':[s.median()-p.median()],\n",
    "        'pop_std':[p.std()], 'samp_std':[s.std()]\n",
    "    })\n",
    "\n",
    "pd.concat([numeric_summary('ITE_ITEM_CURRENT_PRICE'),\n",
    "           numeric_summary('QTY_VISITS')], ignore_index=True)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "1cff0a853dc148c4ae310403150611d9",
   "source": [
    "4. FEATURE ENGINEERING + KNN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "6bbec6e630794ed492d200ceb1e8d856",
   "source": [
    "# ======================================\n",
    "# SIMILITUD DE CELULARES CODIGO FINAL!!\n",
    "# ======================================\n",
    "\n",
    "import re, warnings, math, numpy as np, pandas as pd, time\n",
    "from IPython.display import display\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "TOP_ANCHORS = 30          \n",
    "TOP_K = 3               \n",
    "TEXT_WEIGHT, CAT_WEIGHT, NUM_WEIGHT = 0.55, 0.25, 0.20\n",
    "LEAF_WEIGHT = 0.30        # peso de hojas RF si las usás\n",
    "MAX_TEXT_FEATURES = 8000\n",
    "\n",
    "\n",
    "ALPHA, BETA = 0.85, 0.15\n",
    "\n",
    "WEIGHTS_BONUS = {\n",
    "    'os_match': 1.0,\n",
    "    'series_match': 1.0,\n",
    "    'storage_bin': 0.8,\n",
    "    'ram_bin': 0.6,\n",
    "    'tier_penalty': 0.6,  \n",
    "    'high_end_match': 0.6,\n",
    "    'color_closeness': 0.4\n",
    "}\n",
    "\n",
    "LGB_PARAMS = dict(\n",
    "    n_estimators=20,         # arbolitos\n",
    "    num_leaves=16,           # hojas por árbol\n",
    "    min_child_samples=100,   # datos por hoja\n",
    "    feature_fraction_bynode=0.2,\n",
    "    boosting_type='rf',\n",
    "    subsample=(1.0 - 1.0/np.e),\n",
    "    subsample_freq=1,\n",
    "    feature_fraction=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "def normalize_text(s):\n",
    "    if pd.isna(s): return ''\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[\\-_/]+\",\" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9áéíóúñü\\s]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\",\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "COLOR_MAP = {\n",
    "    'negro':  ['negro','black','grafito','graphite','space gray','gris oscuro','midnight'],\n",
    "    'gris':   ['gris','silver','plateado','titanium','titanio','natural','graphite','space gray'],\n",
    "    'blanco': ['blanco','white'],\n",
    "    'azul':   ['azul','blue','cian'],\n",
    "    'rojo':   ['rojo','red'],\n",
    "    'verde':  ['verde','green'],\n",
    "}\n",
    "def map_color(c):\n",
    "    c0 = normalize_text(c)\n",
    "    if not c0: return np.nan\n",
    "    for base, alts in COLOR_MAP.items():\n",
    "        if c0 == base or any(a in c0 for a in alts):\n",
    "            return base\n",
    "    return c0\n",
    "\n",
    "HIGH_END_BRANDS = {'apple','samsung'}\n",
    "HIGH_END_MODELS_HINTS = {'pro','ultra','s2','s23','s24','s25','max','promax','pro max','plus'}\n",
    "def is_high_end(brand, model):\n",
    "    b = (brand or '').lower(); m = (model or '').lower()\n",
    "    if b in HIGH_END_BRANDS: return 1\n",
    "    return int(any(h in m for h in HIGH_END_MODELS_HINTS))\n",
    "\n",
    "def parse_gb(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r\"(\\d{1,4})\\s*gb\", str(x).lower())\n",
    "    return int(m.group(1)) if m else pd.to_numeric(x, errors='coerce')\n",
    "\n",
    "def extract_storage_from_title(title):\n",
    "    t = normalize_text(title)\n",
    "    m = re.search(r\"(\\d{2,4})\\s?gb\", t)\n",
    "    return int(m.group(1)) if m else np.nan\n",
    "\n",
    "def extract_ram_from_title(title):\n",
    "    t = normalize_text(title)\n",
    "    m = re.search(r\"(\\d{1,2})\\s*gb\\s*ram\", t) or re.search(r\"ram\\s*(\\d{1,2})\\s*gb\", t)\n",
    "    return int(m.group(1)) if m else np.nan\n",
    "\n",
    "def parse_mah(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).lower()\n",
    "    m = re.search(r\"(\\d{3,5})\\s*m?ah\", s) or re.search(r\"(\\d{3,5})\", s)\n",
    "    return int(m.group(1)) if m else pd.to_numeric(x, errors='coerce')\n",
    "\n",
    "def parse_mp(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).lower()\n",
    "    m = re.search(r\"(\\d{1,3}(?:\\.\\d)?)\\s*mp\", s) or re.search(r\"(\\d{1,3}(?:\\.\\d)?)\", s)\n",
    "    try:\n",
    "        return float(m.group(1)) if m else pd.to_numeric(x, errors='coerce')\n",
    "    except Exception:\n",
    "        return pd.to_numeric(x, errors='coerce')\n",
    "\n",
    "def _col(df: pd.DataFrame, name: str):\n",
    "    return df[name] if name in df.columns else pd.Series([np.nan]*len(df), index=df.index)\n",
    "\n",
    "# --------------- FEATURES BASE  ---------------\n",
    "def prepare_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = pd.DataFrame(index=df_in.index)\n",
    "    d['item_id']   = _col(df_in, 'ITE_ITEM_ID')\n",
    "    d['title']     = _col(df_in, 'ITE_ITEM_TITLE')\n",
    "    d['brand']     = _col(df_in, 'BRAND').astype(str).str.strip().str.title()\n",
    "    d['model']     = _col(df_in, 'MODEL').astype(str).str.strip()\n",
    "    d['color']     = _col(df_in, 'COLOR')\n",
    "    d['condition'] = _col(df_in, 'ITEM_CONDITION')\n",
    "    d['price_mxn'] = pd.to_numeric(_col(df_in, 'ITE_ITEM_CURRENT_PRICE'), errors='coerce')\n",
    "    d['visits']    = pd.to_numeric(_col(df_in, 'QTY_VISITS'), errors='coerce')\n",
    "\n",
    "    d['storage_gb'] = _col(df_in, 'INTERNAL_MEMORY').apply(parse_gb)\n",
    "    d['ram_gb']     = _col(df_in, 'RAM').apply(parse_gb)\n",
    "    ms = d['storage_gb'].isna()\n",
    "    if ms.any(): d.loc[ms, 'storage_gb'] = _col(df_in, 'ITE_ITEM_TITLE').apply(extract_storage_from_title)[ms]\n",
    "    mr = d['ram_gb'].isna()\n",
    "    if mr.any(): d.loc[mr, 'ram_gb']     = _col(df_in, 'ITE_ITEM_TITLE').apply(extract_ram_from_title)[mr]\n",
    "\n",
    "    d['battery_mah'] = _col(df_in, 'BATTERY_CAPACITY').apply(parse_mah)\n",
    "    d['front_mp']    = _col(df_in, 'MAIN_FRONT_CAMERA_RESOLUTION').apply(parse_mp)\n",
    "    d['rear_mp']     = _col(df_in, 'MAIN_REAR_CAMERA_RESOLUTION').apply(parse_mp)\n",
    "\n",
    "    d['title_norm']  = d['title'].fillna('').apply(normalize_text)\n",
    "    d['text']        = (d['title_norm'] + ' ' + d['brand'].fillna('').str.lower() + ' ' + d['model'].fillna('').str.lower()).str.strip()\n",
    "    if d['text'].str.strip().eq('').all():\n",
    "        d['text'] = (d['brand'].fillna('') + ' ' + d['model'].fillna('') + ' ' + d['title_norm']).str.strip()\n",
    "\n",
    "    d['color_norm']  = d['color'].apply(map_color)\n",
    "    d['is_high_end'] = [is_high_end(b, m) for b, m in zip(d['brand'], d['model'])]\n",
    "    d['brand_tier']  = np.select(\n",
    "        [d['brand'].str.lower().isin(['apple']),\n",
    "         d['brand'].str.lower().isin(['samsung','google','oneplus'])],\n",
    "        [3, 2], default=1\n",
    "    )\n",
    "\n",
    "    d['log_price']    = np.log1p(d['price_mxn'])\n",
    "    d['price_per_gb'] = (d['price_mxn'] / d['storage_gb']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if d['visits'].isna().all():\n",
    "        d['visits'] = np.random.randint(10_000, 100_000, size=len(d))\n",
    "\n",
    "    return d.reset_index(drop=True)\n",
    "\n",
    "# --------------- FEATURES EXTRA  ---------------\n",
    "def _txt(df):\n",
    "    return (df['title'].fillna('') + ' ' + df['model'].fillna('')).str.lower()\n",
    "\n",
    "def _parse_inches(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s)\n",
    "    m = re.search(r'(\\d{1,2}(?:[.,]\\d)?)\\s*(?:pulg|pulgadas|\")', s)\n",
    "    return float(m.group(1).replace(',', '.')) if m else np.nan\n",
    "\n",
    "def _parse_hz(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    m = re.search(r'(\\d{2,3})\\s*hz', str(s))\n",
    "    return float(m.group(1)) if m else np.nan\n",
    "\n",
    "def _disp_type(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s)\n",
    "    if 'amoled' in s or 'super amoled' in s or 'oled' in s: return 'oled'\n",
    "    if 'ips' in s: return 'ips'\n",
    "    if 'lcd' in s: return 'lcd'\n",
    "    return np.nan\n",
    "\n",
    "def _port_type(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s)\n",
    "    if 'usb c' in s or 'usb-c' in s or 'type c' in s or 'type-c' in s: return 'usb-c'\n",
    "    if 'lightning' in s: return 'lightning'\n",
    "    if 'micro usb' in s or 'micro-usb' in s: return 'micro-usb'\n",
    "    return np.nan\n",
    "\n",
    "def _parse_ip_tier(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s)\n",
    "    m = re.search(r'\\bip(6[0-9])\\b', s)\n",
    "    if not m: return np.nan\n",
    "    code = int(m.group(1))\n",
    "    return {65:0.5, 66:0.75, 67:1.0, 68:2.0}.get(code, np.nan)\n",
    "\n",
    "def _parse_watts(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    m = re.search(r'(\\d{2,3})\\s*w', str(s))\n",
    "    return float(m.group(1)) if m else np.nan\n",
    "\n",
    "def _parse_series(s):\n",
    "    if pd.isna(s): return (np.nan, np.nan, np.nan)\n",
    "    s = str(s)\n",
    "    if 'iphone' in s:\n",
    "        v = 'pro max' if 'pro max' in s else ('pro' if 'pro' in s else ('plus' if 'plus' in s else ('mini' if 'mini' in s else np.nan)))\n",
    "        m = re.search(r'iphone\\s*(se|1[0-9]|[6-9])', s)\n",
    "        gen = m.group(1) if m else np.nan\n",
    "        return ('iphone', gen, v)\n",
    "    if 'galaxy' in s and ' s' in s:\n",
    "        m = re.search(r'galaxy\\s*s\\s*([0-9]{1,2})', s)\n",
    "        gen = m.group(1) if m else np.nan\n",
    "        v = 'ultra' if 'ultra' in s else ('plus' if 'plus' in s else np.nan)\n",
    "        return ('galaxy-s', gen, v)\n",
    "    if 'galaxy' in s and ' a' in s:\n",
    "        m = re.search(r'galaxy\\s*a\\s*([0-9]{1,2})', s)\n",
    "        gen = m.group(1) if m else np.nan\n",
    "        return ('galaxy-a', gen, np.nan)\n",
    "    if 'redmi note' in s:\n",
    "        m = re.search(r'redmi note\\s*([0-9]{1,2})', s)\n",
    "        gen = m.group(1) if m else np.nan\n",
    "        return ('redmi-note', gen, np.nan)\n",
    "    if 'poco' in s:\n",
    "        m = re.search(r'poco\\s*([a-z]?\\d{1,2})', s)\n",
    "        gen = m.group(1) if m else np.nan\n",
    "        return ('poco', gen, np.nan)\n",
    "    if 'moto g' in s:\n",
    "        m = re.search(r'moto g\\s*([0-9]{1,2})', s)\n",
    "        gen = m.group(1) if m else np.nan\n",
    "        return ('moto-g', gen, np.nan)\n",
    "    return (np.nan, np.nan, np.nan)\n",
    "\n",
    "def _parse_soc(s):\n",
    "    if pd.isna(s): return (np.nan, np.nan)\n",
    "    s = str(s)\n",
    "    if 'bionic' in s or re.search(r'\\ba\\d{2}\\b', s):\n",
    "        m = re.search(r'\\ba\\s?(\\d{2})\\b', s); ver = int(m.group(1)) if m else 0\n",
    "        tier = 3 if ver >= 16 else (2 if ver >= 13 else 1)\n",
    "        return ('apple-a', tier)\n",
    "    if 'snapdragon' in s:\n",
    "        if re.search(r'8\\s*gen|8\\d{2}', s): return ('snapdragon', 3)\n",
    "        if re.search(r'7\\s*gen|7\\d{2}', s): return ('snapdragon', 2)\n",
    "        if re.search(r'6\\d{2}|4\\d{2}', s):  return ('snapdragon', 1)\n",
    "        return ('snapdragon', 2)\n",
    "    if 'dimensity' in s:\n",
    "        m = re.search(r'dimensity\\s*(\\d{3,4})', s); n = int(m.group(1)) if m else 0\n",
    "        tier = 3 if n >= 9000 else (2 if n >= 8000 or n >= 700 else 1)\n",
    "        return ('dimensity', tier)\n",
    "    if 'exynos' in s:\n",
    "        m = re.search(r'exynos\\s*(\\d{3,4})', s); n = int(m.group(1)) if m else 0\n",
    "        tier = 3 if n >= 2200 else (2 if n >= 1280 else 1)\n",
    "        return ('exynos', tier)\n",
    "    if 'kirin' in s:\n",
    "        m = re.search(r'kirin\\s*(\\d{3,4})', s); n = int(m.group(1)) if m else 0\n",
    "        tier = 3 if n >= 9000 else (2 if n >= 800 else 1)\n",
    "        return ('kirin', tier)\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "def augment_features_extra(df_feat: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df_feat.copy()\n",
    "    t = _txt(d)\n",
    "    d['screen_in']    = t.apply(_parse_inches)\n",
    "    d['refresh_hz']   = t.apply(_parse_hz)\n",
    "    d['display_type'] = t.apply(_disp_type)\n",
    "    d['screen_class'] = pd.cut(d['screen_in'], bins=[0,6.0,6.3,6.7,10], labels=['S','M','L','XL'])\n",
    "    d['refresh_bin']  = pd.cut(d['refresh_hz'], bins=[0,61,91,121,1000], labels=['60','90','120','144+'])\n",
    "\n",
    "    d['is_5g']     = t.str.contains(r'\\b5g\\b').astype(int)\n",
    "    d['has_esim']  = t.str.contains(r'\\besim\\b').astype(int)\n",
    "    d['has_nfc']   = t.str.contains(r'\\bnfc\\b').astype(int)\n",
    "    d['dual_sim']  = t.str.contains(r'dual[\\s\\-]?sim').astype(int)\n",
    "    d['ip_tier']   = t.apply(_parse_ip_tier)\n",
    "\n",
    "    d['charge_w']   = t.apply(_parse_watts)\n",
    "    d['charge_bin'] = pd.cut(d['charge_w'], bins=[0,19,31,46,66,5000], labels=['<20W','20-30W','31-45W','46-65W','66W+'])\n",
    "    d['port_type']  = t.apply(_port_type)\n",
    "\n",
    "    soc = t.apply(_parse_soc)\n",
    "    d['soc_family'] = soc.apply(lambda x: x[0])\n",
    "    d['soc_tier']   = soc.apply(lambda x: x[1])\n",
    "    d['os_family']  = np.where(d['brand'].str.lower()=='apple', 'iOS', 'Android')\n",
    "\n",
    "    ser = t.apply(_parse_series)\n",
    "    d['series_line'] = ser.apply(lambda x: x[0])\n",
    "    d['series_gen']  = ser.apply(lambda x: x[1])\n",
    "    d['variant']     = ser.apply(lambda x: x[2])\n",
    "\n",
    "    def _nearest(x, grid):\n",
    "        try:\n",
    "            x=float(x); return min(grid, key=lambda g: abs(g-x))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    d['storage_bin'] = d['storage_gb'].apply(lambda x: _nearest(x, [64,128,256,512,1024]))\n",
    "    d['ram_bin']     = d['ram_gb'].apply(lambda x: _nearest(x, [4,6,8,12,16]))\n",
    "    d['battery_bin'] = pd.cut(d['battery_mah'], bins=[0,3800,4500,5000,6000,10000],\n",
    "                              labels=['<3800','3800-4500','4500-5000','5000-6000','6000+'])\n",
    "    try:\n",
    "        d['price_segment'] = pd.qcut(d['price_mxn'], q=5, labels=['P1','P2','P3','P4','P5'])\n",
    "    except Exception:\n",
    "        d['price_segment'] = np.nan\n",
    "    return d\n",
    "\n",
    "# --------------- VECTORIZACIÓN FLEXIBLE ---------------\n",
    "def _ohe_compat(**kwargs):\n",
    "    try:\n",
    "        return OneHotEncoder(sparse_output=True, **{k:v for k,v in kwargs.items() if k!='sparse'})\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(sparse=True, **kwargs)\n",
    "\n",
    "def build_feature_matrix_flex(df_feat: pd.DataFrame,\n",
    "                              text_weight=TEXT_WEIGHT, cat_weight=CAT_WEIGHT, num_weight=NUM_WEIGHT,\n",
    "                              max_text_features=MAX_TEXT_FEATURES,\n",
    "                              extra_cat=None, extra_num=None):\n",
    " \n",
    "    try:\n",
    "        tfidf = TfidfVectorizer(min_df=2, ngram_range=(1,2), max_features=max_text_features, strip_accents='unicode')\n",
    "        X_text = tfidf.fit_transform(df_feat['text'])\n",
    "        if X_text.shape[1] == 0:\n",
    "            raise ValueError(\"Empty vocab\")\n",
    "    except Exception:\n",
    "        tfidf = TfidfVectorizer(min_df=1, ngram_range=(1,1), analyzer='char_wb',\n",
    "                                strip_accents='unicode', max_features=max_text_features//2)\n",
    "        X_text = tfidf.fit_transform(df_feat['text'])\n",
    "\n",
    "    # --- Categóricas ---\n",
    "    base_cat = ['brand','color_norm','condition']\n",
    "    extra_cat = extra_cat or [\n",
    "        'display_type','os_family','series_line','variant','port_type',\n",
    "        'screen_class','refresh_bin','storage_bin','ram_bin','battery_bin',\n",
    "        'charge_bin','soc_family','price_segment'\n",
    "    ]\n",
    "    cat_cols = [c for c in base_cat + extra_cat if c in df_feat.columns]\n",
    "    cat_df = df_feat[cat_cols].copy()\n",
    "    for c in cat_cols:\n",
    "        cat_df[c] = cat_df[c].astype('object')   # <- rompe Categorical\n",
    "    cat_df = cat_df.fillna('NA')\n",
    "\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=True)\n",
    "    X_cat = ohe.fit_transform(cat_df)\n",
    "\n",
    "    # --- Numéricas ---\n",
    "    base_num = ['log_price','storage_gb','ram_gb','battery_mah','front_mp','rear_mp','is_high_end','brand_tier','price_per_gb']\n",
    "    extra_num = extra_num or ['screen_in','refresh_hz','ip_tier','charge_w','soc_tier']\n",
    "    num_cols = [c for c in base_num + extra_num if c in df_feat.columns]\n",
    "    num_df = df_feat[num_cols].copy()\n",
    "    num_df = num_df.fillna(num_df.median(numeric_only=True))\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    X_num = scaler.fit_transform(num_df)\n",
    "\n",
    "    # --- Mezcla ---\n",
    "    from scipy.sparse import hstack\n",
    "    X = hstack([X_text * text_weight, X_cat * cat_weight, X_num * num_weight], format='csr')\n",
    "    print(f\"[FLEX] text:{X_text.shape} cat:{X_cat.shape} num:{X_num.shape} -> X:{X.shape}\")\n",
    "    arts = {'tfidf':tfidf,'ohe':ohe,'scaler':scaler,'cat_cols':cat_cols,'num_cols':num_cols}\n",
    "    return X, arts\n",
    "\n",
    "\n",
    "    X = hstack([X_text * text_weight, X_cat * cat_weight, X_num * num_weight], format='csr')\n",
    "    print(f\"[FLEX] text:{X_text.shape} cat:{X_cat.shape} num:{X_num.shape} -> X:{X.shape}\")\n",
    "    arts = {'tfidf':tfidf,'ohe':ohe,'scaler':scaler,'cat_cols':cat_cols,'num_cols':num_cols}\n",
    "    return X, arts\n",
    "\n",
    "# --------------- HOJAS RF con LightGBM  ---------------\n",
    "def build_rf_leaf_features(df_feat: pd.DataFrame, params=None, target_col='visits', use_log_target=True):\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ lightgbm no está instalado. Saltando FE por hojas. Error:\", e)\n",
    "        return None, None\n",
    "    cfg = dict(LGB_PARAMS); \n",
    "    if params: cfg.update(params)\n",
    "\n",
    "    num_cols = ['log_price','storage_gb','ram_gb','battery_mah','front_mp','rear_mp','is_high_end','brand_tier','price_per_gb']\n",
    "    cat_cols = ['brand','color_norm','condition']\n",
    "    cols = [c for c in num_cols+cat_cols if c in df_feat.columns]\n",
    "    if not cols:\n",
    "        print(\"⚠️ No hay cols disponibles para LightGBM.\")\n",
    "        return None, None\n",
    "\n",
    "    X_lgb = df_feat[cols].copy()\n",
    "    for c in cat_cols:\n",
    "        if c in X_lgb.columns: X_lgb[c] = X_lgb[c].astype('category')\n",
    "\n",
    "    y = df_feat[target_col].astype(float)\n",
    "    if use_log_target: y = np.log1p(y.clip(lower=0))\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        boosting_type=cfg['boosting_type'],\n",
    "        n_estimators=cfg['n_estimators'],\n",
    "        num_leaves=cfg['num_leaves'],\n",
    "        min_child_samples=cfg['min_child_samples'],\n",
    "        feature_fraction_bynode=cfg['feature_fraction_bynode'],\n",
    "        subsample=cfg['subsample'],\n",
    "        subsample_freq=cfg['subsample_freq'],\n",
    "        feature_fraction=cfg['feature_fraction'],\n",
    "        random_state=cfg['random_state'],\n",
    "        n_jobs=cfg['n_jobs'],\n",
    "        objective='regression'\n",
    "    )\n",
    "    model.fit(X_lgb, y, categorical_feature=[c for c in cat_cols if c in X_lgb.columns])\n",
    "    leaves = model.predict(X_lgb, pred_leaf=True)  # (n_samples, n_trees)\n",
    "    leaf_cols = [f\"rf_leaf_{j:03d}\" for j in range(leaves.shape[1])]\n",
    "    leaf_df = pd.DataFrame({leaf_cols[j]: leaves[:, j].astype('int32').astype(str) for j in range(leaves.shape[1])})\n",
    "\n",
    "    ohe_leaf = _ohe_compat(handle_unknown='ignore')\n",
    "    X_leaf = ohe_leaf.fit_transform(leaf_df)\n",
    "    print(f\"[RF-LEAVES] trees:{leaves.shape[1]} -> X_leaf:{X_leaf.shape}\")\n",
    "    arts = {'lgbm_model': model, 'leaf_ohe': ohe_leaf, 'leaf_cols': leaf_cols}\n",
    "    return X_leaf, arts\n",
    "\n",
    "def build_final_matrix(df_feat, add_rf_leaves=True, leaf_weight=LEAF_WEIGHT, rf_params=None):\n",
    "    X_base, base_arts = build_feature_matrix_flex(df_feat, text_weight=TEXT_WEIGHT, cat_weight=CAT_WEIGHT, num_weight=NUM_WEIGHT, max_text_features=MAX_TEXT_FEATURES)\n",
    "    if add_rf_leaves:\n",
    "        X_leaf, leaf_arts = build_rf_leaf_features(df_feat, params=rf_params or LGB_PARAMS)\n",
    "    else:\n",
    "        X_leaf, leaf_arts = (None, None)\n",
    "    if X_leaf is None:\n",
    "        print(\"[TOTAL] X:\", X_base.shape, \"(sin hojas RF)\")\n",
    "        return X_base, {'base':base_arts, 'rf_leaves':None}\n",
    "    X_total = hstack([X_base, X_leaf * leaf_weight], format='csr')\n",
    "    print(f\"[TOTAL] X_base:{X_base.shape} + X_leaf:{X_leaf.shape} -> X_total:{X_total.shape}\")\n",
    "    return X_total, {'base':base_arts, 'rf_leaves':leaf_arts}\n",
    "\n",
    "# --------------- kNN + RE-RANK HEURÍSTICO ---------------\n",
    "def color_closeness(a, b):\n",
    "    if pd.isna(a) or pd.isna(b): return 0.0\n",
    "    a, b = str(a).lower(), str(b).lower()\n",
    "    if a == b: return 1.0\n",
    "\n",
    "    pairs = {('negro','gris'), ('gris','negro')}\n",
    "    return 0.8 if (a,b) in pairs else 0.0\n",
    "\n",
    "def hybrid_rerank_score(a, b, sim):\n",
    "   \n",
    "    bonus = 0.0\n",
    "    bonus += WEIGHTS_BONUS['os_match'] * (1.0 if a['os_family'] == b['os_family'] else 0.0)\n",
    "    bonus += WEIGHTS_BONUS['series_match'] * (1.0 if (a['series_line']==b['series_line']) else 0.0)\n",
    "    bonus += WEIGHTS_BONUS['storage_bin'] * (1.0 if (a.get('storage_bin') == b.get('storage_bin')) else 0.0)\n",
    "    bonus += WEIGHTS_BONUS['ram_bin'] * (1.0 if (a.get('ram_bin') == b.get('ram_bin')) else 0.0)\n",
    "    tier_diff = abs((a.get('brand_tier') or 0) - (b.get('brand_tier') or 0))\n",
    "    bonus += WEIGHTS_BONUS['tier_penalty'] * (1.0 - min(1.0, tier_diff*0.5))  \n",
    "    bonus += WEIGHTS_BONUS['high_end_match'] * (1.0 if (a.get('is_high_end') == b.get('is_high_end')) else 0.0)\n",
    "    bonus += WEIGHTS_BONUS['color_closeness'] * color_closeness(a.get('color_norm'), b.get('color_norm'))\n",
    "    return ALPHA*sim + BETA*bonus\n",
    "\n",
    "def build_pairs_table(df_feat, X, top_anchors=TOP_ANCHORS, top_k=TOP_K):\n",
    "    nn = NearestNeighbors(metric='cosine', algorithm='auto', n_neighbors=min(50, X.shape[0]))\n",
    "    t0 = time.time(); nn.fit(X); fit_s = time.time() - t0\n",
    "\n",
    "    anchors_idx = df_feat.sort_values('visits', ascending=False).head(top_anchors).index.to_list()\n",
    "    dist, idx = nn.kneighbors(X[anchors_idx], return_distance=True)\n",
    "    rows = []\n",
    "    for row_i, aidx in enumerate(anchors_idx):\n",
    "        \n",
    "        cand_idx = [int(c) for c in idx[row_i, 1:]]\n",
    "        cand_dist = [float(d) for d in dist[row_i, 1:]]\n",
    "    \n",
    "        scored = []\n",
    "        arow = df_feat.iloc[aidx]\n",
    "        for c, d in zip(cand_idx, cand_dist):\n",
    "            brow = df_feat.iloc[c]\n",
    "            sim = 1.0 - d\n",
    "            score = hybrid_rerank_score(arow, brow, sim)\n",
    "            scored.append((score, sim, c))\n",
    "        scored.sort(reverse=True)  # mayor score primero\n",
    "        for score, sim, c in scored[:top_k]:\n",
    "            a = arow; b = df_feat.iloc[c]\n",
    "            rows.append({\n",
    "                'anchor_item_id': a['item_id'], 'sim_item_id': b['item_id'],\n",
    "                'anchor_title': a['title'],     'sim_title': b['title'],\n",
    "                'anchor_brand': a['brand'],     'sim_brand': b['brand'],\n",
    "                'anchor_model': a['model'],     'sim_model': b['model'],\n",
    "                'anchor_color': a['color'],     'sim_color': b['color'],\n",
    "                'anchor_storage_gb': a['storage_gb'], 'sim_storage_gb': b['storage_gb'],\n",
    "                'anchor_ram_gb': a['ram_gb'],         'sim_ram_gb': b['ram_gb'],\n",
    "                'anchor_price_mxn': a['price_mxn'],   'sim_price_mxn': b['price_mxn'],\n",
    "                'anchor_visits': a['visits'],         'sim_visits': b['visits'],\n",
    "                'similarity': round(sim, 4), 'hybrid_score': round(score, 4)\n",
    "            })\n",
    "    pairs_df = pd.DataFrame(rows)\n",
    "    print(f\"⏱️ fit={fit_s:.3f}s | pares={len(pairs_df)} (top_{top_k} por {top_anchors} anchors)\")\n",
    "    return pairs_df\n",
    "\n",
    "\n",
    "def bench_knn(df_feat, X, Ns=(2000, 5000, 10000, 20000, None), anchors_k=30, k_query=6):\n",
    "    out=[]\n",
    "    for N in Ns:\n",
    "        if N is None: N = X.shape[0]\n",
    "        idx_keep = df_feat.sort_values('visits', ascending=False).head(N).index.to_list()\n",
    "        XN\n",
    "\n",
    "\n",
    "\n",
    "def run_similarity_pipeline(base_df, top_anchors=30, top_k=3, add_rf=True):\n",
    "    assert base_df is not None and len(base_df) > 0, \"Pasá tu DataFrame en base_df (p.ej., df)\"\n",
    "    print(\">>> Arrancando pipeline de similitud\")\n",
    "\n",
    "    # 1) Features\n",
    "    df_feat = prepare_features(base_df)\n",
    "    df_feat = augment_features_extra(df_feat)\n",
    "    print(\"[1] df_feat:\", df_feat.shape)\n",
    "    try:\n",
    "        display(df_feat.head(3))\n",
    "    except Exception:\n",
    "        print(df_feat.head(3).to_string(index=False))\n",
    "\n",
    "    # 2) Matriz X (con o sin hojas RF)\n",
    "    X, arts = build_final_matrix(\n",
    "        df_feat,\n",
    "        add_rf_leaves=add_rf,       \n",
    "        leaf_weight=LEAF_WEIGHT,\n",
    "        rf_params=LGB_PARAMS\n",
    "    )\n",
    "    print(\"[2] X:\", X.shape)\n",
    "\n",
    "    # vcinos + tabla de pares\n",
    "    pairs = build_pairs_table(df_feat, X, top_anchors=top_anchors, top_k=top_k)\n",
    "    print(\"[3] Pares:\", len(pairs))\n",
    "    try:\n",
    "        display(pairs.head(10))\n",
    "    except Exception:\n",
    "        print(pairs.head(10).to_string(index=False))\n",
    "\n",
    "\n",
    "    out_csv = \"output_pares_similares_top30_topK.csv\"\n",
    "    pairs.to_csv(out_csv, index=False)\n",
    "    print(f\"✅ Exportado: {out_csv}\")\n",
    "\n",
    "    return df_feat, X, pairs\n",
    "\n",
    "\n",
    "BASE = globals().get('df_bq', None) or globals().get('df', None)\n",
    "if BASE is None:\n",
    "    raise NameError(\"No encontré 'df' ni 'df_bq'. Cargá tu DataFrame (el de la query) en una variable llamada df.\")\n",
    "\n",
    "\n",
    "print(\"Columnas de BASE:\", sorted(list(BASE.columns)))\n",
    "\n",
    "\n",
    "df_feat, X, pairs = run_similarity_pipeline(BASE, top_anchors=30, top_k=3, add_rf=True)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "efa860d6d22147b3a7f5e1b8d07bafb6",
   "source": [
    "6. TIEMPOS Y ESCALABILIDAD\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "07e8d1acfad44c049e9aca8c1f8f19ca",
   "source": [
    "# BENCHMARK DE ESCALABILIDAD \n",
    "\n",
    "import time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def _csr_bytes(csr):\n",
    "    \n",
    "    return (csr.data.nbytes + csr.indices.nbytes + csr.indptr.nbytes)\n",
    "\n",
    "def benchmark_knn_scalability(df_feat, X,\n",
    "                              Ns=(2000, 5000, 10000, 20000, None),\n",
    "                              anchors_k=30, k_query=6,\n",
    "                              algorithm='brute'):\n",
    "    \"\"\"\n",
    "    Mide tiempo de fit y de consulta (kNN) para distintos tamaños N.\n",
    "    - df_feat: dataframe con columna 'visits' para elegir anchors\n",
    "    - X: matriz sparse (todas las filas)\n",
    "    - Ns: lista de N. Usar None como 'usar todo'\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    order_idx = df_feat.sort_values('visits', ascending=False).index.to_list()\n",
    "\n",
    "    for N in Ns:\n",
    "        N_eff = X.shape[0] if N is None else int(N)\n",
    "        keep_idx = order_idx[:N_eff]\n",
    "      \n",
    "        pos = {idx:i for i, idx in enumerate(keep_idx)}\n",
    "\n",
    "        XN = X[keep_idx]\n",
    "       \n",
    "        anchors_idx = keep_idx[:min(anchors_k, N_eff)]\n",
    "        anchors_pos = [pos[i] for i in anchors_idx]\n",
    "\n",
    "      \n",
    "        t0 = time.perf_counter()\n",
    "        nn = NearestNeighbors(metric='cosine', algorithm=algorithm, n_neighbors=min(k_query+1, N_eff))\n",
    "        nn.fit(XN)\n",
    "        fit_s = time.perf_counter() - t0\n",
    "\n",
    "      \n",
    "        t1 = time.perf_counter()\n",
    "        _ = nn.kneighbors(XN[anchors_pos], n_neighbors=min(k_query+1, N_eff), return_distance=True)\n",
    "        query_s = time.perf_counter() - t1\n",
    "\n",
    "        mem_mb = _csr_bytes(XN) / (1024**2)\n",
    "        results.append({\n",
    "            'N': N_eff,\n",
    "            'fit_seconds': fit_s,\n",
    "            'query_seconds_total': query_s,\n",
    "            'query_seconds_per_anchor': query_s / max(1, len(anchors_pos)),\n",
    "            'memory_MB': mem_mb\n",
    "        })\n",
    "        print(f\"N={N_eff:>7d} | fit={fit_s:6.3f}s | query_total={query_s:6.3f}s \"\n",
    "              f\"| query/anchor={query_s/max(1,len(anchors_pos)):.4f}s | mem≈{mem_mb:,.1f} MB\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "if 'df_feat' not in globals() or 'X' not in globals():\n",
    "    BASE = globals().get('df_bq', None) or globals().get('df', None)\n",
    "    assert BASE is not None and len(BASE) > 0, \"Cargá tu DataFrame en df o df_bq\"\n",
    "    df_feat = prepare_features(BASE)\n",
    "    df_feat = augment_features_extra(df_feat)\n",
    "    X, _ = build_feature_matrix_flex(df_feat) \n",
    "\n",
    "Ns = (2000, 5000, 10000, 20000, None)  \n",
    "bench = benchmark_knn_scalability(df_feat, X, Ns=Ns, anchors_k=30, k_query=6, algorithm='brute')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(bench['N'], bench['fit_seconds'], marker='o', label='fit')\n",
    "plt.plot(bench['N'], bench['query_seconds_total'], marker='o', label='query total (30 anchors)')\n",
    "plt.xlabel('Tamaño N')\n",
    "plt.ylabel('Segundos')\n",
    "plt.title('Escalabilidad kNN (cosine, sparse)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(bench['N'], bench['query_seconds_per_anchor'], marker='o')\n",
    "plt.xlabel('Tamaño N')\n",
    "plt.ylabel('Segundos por anchor')\n",
    "plt.title('Tiempo de consulta por anchor vs N')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTabla resumen:\")\n",
    "display(bench)\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}